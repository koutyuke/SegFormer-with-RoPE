{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sq3ffU2BHRa2"
   },
   "source": [
    "# SegFormer with RoPE model at b0 and 512px input images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mode: axial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSi_l0w3Hg8w"
   },
   "source": [
    "# 0. imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6098,
     "status": "ok",
     "timestamp": 1737735693265,
     "user": {
      "displayName": "蓬莱研（HORNET）",
      "userId": "04229989103698065141"
     },
     "user_tz": -540
    },
    "id": "lWGAQ8uTHkRo",
    "outputId": "ce25457c-742a-4564-9101-063b9ba5c916"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "pip install -q datasets transformers evaluate icecream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zG0aHvbcHlWh"
   },
   "outputs": [],
   "source": [
    "!pip list | grep datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BNJVxBfqHpAx"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kousuke/Desktop/programs/python/SegFormer-with-RoPE/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import hf_hub_download\n",
    "from icecream import ic\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import ColorJitter\n",
    "from tqdm.notebook import trange\n",
    "from transformers import (\n",
    "  AutoImageProcessor,\n",
    "  AutoModelForSemanticSegmentation,\n",
    "  SegformerConfig,\n",
    "  SegformerForSemanticSegmentation,\n",
    "  SegformerImageProcessor,\n",
    "  SegformerModel,\n",
    "  Trainer,\n",
    "  TrainingArguments,\n",
    ")\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import BaseModelOutput\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lj6R_OQBHs54"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"./drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAaZ8YSYH-Wz"
   },
   "source": [
    "# 1. dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Tex_pkPbIIYR"
   },
   "outputs": [],
   "source": [
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2nas_dtwIKyp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| dataset: DatasetDict({\n",
      "                 train: Dataset({\n",
      "                     features: ['image', 'annotation', 'scene_category'],\n",
      "                     num_rows: 20210\n",
      "                 })\n",
      "                 test: Dataset({\n",
      "                     features: ['image', 'annotation', 'scene_category'],\n",
      "                     num_rows: 3352\n",
      "                 })\n",
      "                 validation: Dataset({\n",
      "                     features: ['image', 'annotation', 'scene_category'],\n",
      "                     num_rows: 2000\n",
      "                 })\n",
      "             })\n",
      "ic| train_dataset: Dataset({\n",
      "                       features: ['image', 'annotation', 'scene_category'],\n",
      "                       num_rows: 20210\n",
      "                   })\n",
      "ic| validation_dataset: Dataset({\n",
      "                            features: ['image', 'annotation', 'scene_category'],\n",
      "                            num_rows: 1000\n",
      "                        })\n"
     ]
    }
   ],
   "source": [
    "# dataset = dataset.shuffle(seed=42).select(range(10000)).train_test_split(test_size=0.1)\n",
    "# train_dataset = dataset[\"train\"]\n",
    "# test_dataset = dataset[\"test\"]\n",
    "\n",
    "dataset = load_dataset(\"scene_parse_150\")\n",
    "ic(dataset)\n",
    "train_dataset = dataset[\"train\"].shuffle(seed=42)\n",
    "validation_dataset = dataset[\"validation\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "ic(train_dataset)\n",
    "ic(validation_dataset);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k18kwefbIgLO"
   },
   "source": [
    "# 2. Preproces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_0QhVIKGIiP-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SegformerImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_reduce_labels\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"SegformerImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 512,\n",
       "    \"width\": 512\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)\n",
    "image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PtxsWa7wIobS"
   },
   "outputs": [],
   "source": [
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n",
    "\n",
    "\n",
    "def train_transforms(example_batch):\n",
    "  images = [jitter((x).convert(\"RGB\")) if x.mode == \"L\" else jitter(x) for x in example_batch[\"image\"]]\n",
    "  labels = list(example_batch[\"annotation\"])\n",
    "  inputs = image_processor(images, labels)\n",
    "  return inputs\n",
    "\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "  images = [x.convert(\"RGB\") if x.mode == \"L\" else x for x in example_batch[\"image\"]]\n",
    "  labels = list(example_batch[\"annotation\"])\n",
    "\n",
    "  inputs = image_processor(images, labels)\n",
    "  return inputs\n",
    "\n",
    "\n",
    "train_dataset.set_transform(train_transforms)\n",
    "validation_dataset.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| validation_dataset[0][\"labels\"]: array([[255, 255, 255, ..., 255, 255, 255],\n",
      "                                            [255, 255, 255, ...,   2,   2,   2],\n",
      "                                            [255,   2,   2, ...,   2,   2,   2],\n",
      "                                            ...,\n",
      "                                            [255,  11,  11, ...,  11,  11, 255],\n",
      "                                            [255,  11,  11, ...,  11,  11, 255],\n",
      "                                            [255, 255, 255, ...,  11,  11, 255]], shape=(512, 512))\n",
      "ic| train_dataset[0][\"labels\"].shape: (512, 512)\n"
     ]
    }
   ],
   "source": [
    "ic(validation_dataset[0][\"labels\"])\n",
    "ic(train_dataset[0][\"labels\"].shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TD9a05GJIxDJ"
   },
   "source": [
    "# 3. Compute Metorics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWhn20GOI70H"
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  with torch.no_grad():\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.astype(np.uint8)\n",
    "\n",
    "    batch_size = 100\n",
    "    num_samples = logits.shape[0]\n",
    "\n",
    "    pred_labels = []\n",
    "\n",
    "    for i in trange(0, num_samples, batch_size, leave=False):\n",
    "      logits_batch = logits[i : i + batch_size]\n",
    "      logits_batch = torch.from_numpy(logits_batch)\n",
    "      logits_batch = nn.functional.interpolate(\n",
    "        logits_batch,\n",
    "        size=labels.shape[-2:],\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False,\n",
    "      )\n",
    "      pred_labels.append(logits_batch.argmax(dim=1).detach().cpu().numpy().astype(np.uint8))\n",
    "    pred_labels = np.concatenate(pred_labels, axis=0)\n",
    "\n",
    "    metrics = metric.compute(\n",
    "      predictions=pred_labels,\n",
    "      references=labels,\n",
    "      num_labels=num_labels,\n",
    "      ignore_index=255,\n",
    "      reduce_labels=False,\n",
    "    )\n",
    "\n",
    "    keys_to_remove = [key for key, value in metrics.items() if isinstance(value, np.ndarray)]\n",
    "    for key in keys_to_remove:\n",
    "      del metrics[key]\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9oC8u3TUJB6w"
   },
   "source": [
    "# 4. Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izXcrXWHJG5W"
   },
   "source": [
    "## 4.1 実装\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdQk7NfeJMMk"
   },
   "outputs": [],
   "source": [
    "# reference: transformers.segformer\n",
    "\n",
    "\n",
    "def init_random_2d_freqs(dim: int, num_heads: int, theta: float = 10.0, rotate: bool = True):\n",
    "  freqs_x = []  # (num_heads, dim//4)\n",
    "  freqs_y = []  # (num_heads, dim//4)\n",
    "  mag = 1 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))  # (dim//4)\n",
    "  for _ in range(num_heads):\n",
    "    angles = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)  # (1)\n",
    "    fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(torch.pi / 2 + angles)], dim=-1)  # (dim//2)\n",
    "    fy = torch.cat([mag * torch.sin(angles), mag * torch.sin(torch.pi / 2 + angles)], dim=-1)\n",
    "    freqs_x.append(fx)\n",
    "    freqs_y.append(fy)\n",
    "  freqs_x = torch.stack(freqs_x, dim=0)\n",
    "  freqs_y = torch.stack(freqs_y, dim=0)\n",
    "  freqs = torch.stack([freqs_x, freqs_y], dim=0)\n",
    "  return freqs  # (2, num_heads, dim // 2)\n",
    "\n",
    "\n",
    "def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n",
    "  \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "\n",
    "  Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n",
    "  however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n",
    "  See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n",
    "  layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n",
    "  argument.\n",
    "  \"\"\"\n",
    "  if drop_prob == 0.0 or not training:\n",
    "    return input\n",
    "  keep_prob = 1 - drop_prob\n",
    "  shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "  random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n",
    "  random_tensor.floor_()  # binarize\n",
    "  output = input.div(keep_prob) * random_tensor\n",
    "  return output\n",
    "\n",
    "\n",
    "def compute_mixed_cis(freqs: torch.Tensor, t_x: torch.Tensor, t_y: torch.Tensor, num_heads: int):\n",
    "  N = t_x.shape[0]\n",
    "  depth = freqs.shape[1]  # (2, len(self.blocks), C//2)[1]\n",
    "  # No float 16 for this range\n",
    "  with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "    freqs_x = (\n",
    "      (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2))\n",
    "      # (N, depth, C//2)\n",
    "      .view(depth, N, num_heads, -1)  # (depth, N, num_heads, C_per_head//2)\n",
    "      .permute(0, 2, 1, 3)\n",
    "    )\n",
    "    freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y)\n",
    "\n",
    "  return freqs_cis  # (depth, num_heads, N, dim//2)\n",
    "\n",
    "\n",
    "def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 100.0):\n",
    "  freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "  freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "\n",
    "  t_x, t_y = init_t_xy(end_x, end_y)\n",
    "  freqs_x = torch.outer(t_x, freqs_x)\n",
    "  freqs_y = torch.outer(t_y, freqs_y)\n",
    "  freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
    "  freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
    "  return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
    "\n",
    "\n",
    "def init_t_xy(end_x: int, end_y: int):\n",
    "  t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
    "  t_x = (t % end_x).float()\n",
    "  t_y = torch.div(t, end_x, rounding_mode=\"floor\").float()\n",
    "  return t_x, t_y\n",
    "\n",
    "\n",
    "class SegformerSelfOutput(nn.Module):\n",
    "  def __init__(self, config, hidden_size):\n",
    "    super().__init__()\n",
    "    self.dense = nn.Linear(hidden_size, hidden_size)\n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "  def forward(self, hidden_states, input_tensor):\n",
    "    hidden_states = self.dense(hidden_states)\n",
    "    hidden_states = self.dropout(hidden_states)\n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "class SegformerDropPath(nn.Module):\n",
    "  \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n",
    "\n",
    "  def __init__(self, drop_prob: float | None = None) -> None:\n",
    "    super().__init__()\n",
    "    self.drop_prob = drop_prob\n",
    "\n",
    "  def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "    return drop_path(hidden_states, self.drop_prob, self.training)\n",
    "\n",
    "  def extra_repr(self) -> str:\n",
    "    return \"p={}\".format(self.drop_prob)\n",
    "\n",
    "\n",
    "class SegformerOverlapPatchEmbeddings(nn.Module):\n",
    "  \"\"\"Construct the overlapping patch embeddings.\"\"\"\n",
    "\n",
    "  def __init__(self, patch_size, stride, num_channels, hidden_size):\n",
    "    super().__init__()\n",
    "    self.proj = nn.Conv2d(\n",
    "      num_channels,\n",
    "      hidden_size,\n",
    "      kernel_size=patch_size,\n",
    "      stride=stride,\n",
    "      padding=patch_size // 2,\n",
    "    )\n",
    "\n",
    "    self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "  def forward(self, pixel_values):\n",
    "    embeddings = self.proj(pixel_values)\n",
    "    _, _, height, width = embeddings.shape\n",
    "    # (batch_size, num_channels, height, width) -> (batch_size, num_channels, height*width) -> (batch_size, height*width, num_channels)\n",
    "    # this can be fed to a Transformer layer\n",
    "    embeddings = embeddings.flatten(2).transpose(1, 2)\n",
    "    embeddings = self.layer_norm(embeddings)\n",
    "    return embeddings, height, width\n",
    "\n",
    "\n",
    "class SegformerDWConv(nn.Module):\n",
    "  def __init__(self, dim=768):\n",
    "    super().__init__()\n",
    "    self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "  def forward(self, hidden_states, height, width):\n",
    "    batch_size, seq_len, num_channels = hidden_states.shape\n",
    "    hidden_states = hidden_states.transpose(1, 2).view(batch_size, num_channels, height, width)\n",
    "    hidden_states = self.dwconv(hidden_states)\n",
    "    hidden_states = hidden_states.flatten(2).transpose(1, 2)\n",
    "\n",
    "    return hidden_states\n",
    "\n",
    "\n",
    "class SegformerMixFFN(nn.Module):\n",
    "  def __init__(self, config, in_features, hidden_features=None, out_features=None):\n",
    "    super().__init__()\n",
    "    out_features = out_features or in_features\n",
    "    self.dense1 = nn.Linear(in_features, hidden_features)\n",
    "    self.dwconv = SegformerDWConv(hidden_features)\n",
    "    if isinstance(config.hidden_act, str):\n",
    "      self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "    else:\n",
    "      self.intermediate_act_fn = config.hidden_act\n",
    "    self.dense2 = nn.Linear(hidden_features, out_features)\n",
    "    self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "  def forward(self, hidden_states, height, width):\n",
    "    hidden_states = self.dense1(hidden_states)\n",
    "    hidden_states = self.dwconv(hidden_states, height, width)\n",
    "    hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "    hidden_states = self.dropout(hidden_states)\n",
    "    hidden_states = self.dense2(hidden_states)\n",
    "    hidden_states = self.dropout(hidden_states)\n",
    "    return hidden_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1maFAaXyJUXf"
   },
   "outputs": [],
   "source": [
    "# reference: vit-rope\n",
    "\n",
    "\n",
    "def init_random_2d_freqs(dim: int, num_heads: int, theta: float = 10.0, rotate: bool = True):\n",
    "  freqs_x = []  # (num_heads, dim//4)\n",
    "  freqs_y = []  # (num_heads, dim//4)\n",
    "  mag = 1 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))  # (dim//4)\n",
    "  for _ in range(num_heads):\n",
    "    angles = torch.rand(1) * 2 * torch.pi if rotate else torch.zeros(1)  # (1)\n",
    "    fx = torch.cat([mag * torch.cos(angles), mag * torch.cos(torch.pi / 2 + angles)], dim=-1)  # (dim//2)\n",
    "    fy = torch.cat([mag * torch.sin(angles), mag * torch.sin(torch.pi / 2 + angles)], dim=-1)\n",
    "    freqs_x.append(fx)\n",
    "    freqs_y.append(fy)\n",
    "  freqs_x = torch.stack(freqs_x, dim=0)\n",
    "  freqs_y = torch.stack(freqs_y, dim=0)\n",
    "  freqs = torch.stack([freqs_x, freqs_y], dim=0)\n",
    "  return freqs  # (2, num_heads, dim // 2)\n",
    "\n",
    "\n",
    "def compute_mixed_cis(freqs: torch.Tensor, t_x: torch.Tensor, t_y: torch.Tensor, num_heads: int):\n",
    "  N = t_x.shape[0]\n",
    "  depth = freqs.shape[1]  # (2, len(self.blocks), C//2)[1]\n",
    "  # No float 16 for this range\n",
    "  with torch.amp.autocast(device_type=\"cuda\", enabled=False):\n",
    "    freqs_x = (\n",
    "      (t_x.unsqueeze(-1) @ freqs[0].unsqueeze(-2))\n",
    "      # (N, depth, C//2)\n",
    "      .view(depth, N, num_heads, -1)  # (depth, N, num_heads, C_per_head//2)\n",
    "      .permute(0, 2, 1, 3)\n",
    "    )\n",
    "    freqs_y = (t_y.unsqueeze(-1) @ freqs[1].unsqueeze(-2)).view(depth, N, num_heads, -1).permute(0, 2, 1, 3)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs_x), freqs_x + freqs_y)\n",
    "\n",
    "  return freqs_cis  # (depth, num_heads, N, dim//2)\n",
    "\n",
    "\n",
    "def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 100.0):\n",
    "  freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "  freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))\n",
    "\n",
    "  t_x, t_y = init_t_xy(end_x, end_y)\n",
    "  freqs_x = torch.outer(t_x, freqs_x)\n",
    "  freqs_y = torch.outer(t_y, freqs_y)\n",
    "  freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)\n",
    "  freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)\n",
    "  return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)\n",
    "\n",
    "\n",
    "def init_t_xy(end_x: int, end_y: int):\n",
    "  t = torch.arange(end_x * end_y, dtype=torch.float32)\n",
    "  t_x = (t % end_x).float()\n",
    "  t_y = torch.div(t, end_x, rounding_mode=\"floor\").float()\n",
    "  return t_x, t_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNUTQ3SKJcxV"
   },
   "outputs": [],
   "source": [
    "# 実装\n",
    "\n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "  # (B, num_heads, N, C_per_head // 2)\n",
    "  # freqs_cis (N, C_per_head) or (num_heads, N, C_per_head//2)\n",
    "\n",
    "  ndim = x.ndim\n",
    "\n",
    "  if ndim <= 1:\n",
    "    raise ValueError(\"ndim must be greater than 1\")\n",
    "\n",
    "  if freqs_cis.shape == (x.shape[-2], x.shape[-1]):\n",
    "    shape = [d if i >= ndim - 2 else 1 for i, d in enumerate(x.shape)]\n",
    "  elif freqs_cis.shape == (x.shape[-3], x.shape[-2], x.shape[-1]):\n",
    "    shape = [d if i >= ndim - 3 else 1 for i, d in enumerate(x.shape)]\n",
    "  else:\n",
    "    msg = f\"Invalid shape for `freqs_cis {freqs_cis.shape}` and `x {x.shape}`\"\n",
    "    raise ValueError(msg)\n",
    "\n",
    "  return freqs_cis.view(*shape)  # (1, 1 or num_heads, N, C_per_head//2)\n",
    "\n",
    "\n",
    "def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, sr_ratio: int, freqs_cis: torch.Tensor, height, width):\n",
    "  # freqs_cis (N, C_per_head) or (num_heads, N, C_per_head//2)\n",
    "\n",
    "  xq_ = torch.view_as_complex(\n",
    "    xq.float().reshape(*xq.shape[:-1], -1, 2)\n",
    "  )  # (B, num_heads, N, C_per_head) -> (B, num_heads, N, C_per_head//2)\n",
    "  xk_ = torch.view_as_complex(\n",
    "    xk.float().reshape(*xk.shape[:-1], -1, 2)\n",
    "  )  # (B, num_heads, H*W/sr^2, C_per_head) -> (B, num_heads, H*W/sr^2, C_per_head//2)\n",
    "\n",
    "  xq_freqs_cis = reshape_for_broadcast(freqs_cis, xq_)  # (1, 1 or num_heads, N, C_per_head//2)\n",
    "  xk_freqs_cis = xq_freqs_cis  # (1, 1 or num_heads, N, C_per_head//2)\n",
    "\n",
    "  if sr_ratio > 1:\n",
    "    f_b, f_head, _, f_c = xq_freqs_cis.shape\n",
    "\n",
    "    xk_freqs_cis = xk_freqs_cis.view(f_b, f_head, height, width, f_c).permute(\n",
    "      0, 1, 4, 2, 3\n",
    "    )  # (1, 1 or num_heads, C_per_head//2, H, W)\n",
    "\n",
    "    xk_freqs_cis = xk_freqs_cis.view(f_b * f_head, f_c, height, width)\n",
    "    # (1, 1 or num_heads, C_per_head//2, H, W)\n",
    "\n",
    "    # === pooling ===\n",
    "\n",
    "    real_part = xk_freqs_cis.real\n",
    "    imag_part = xk_freqs_cis.imag\n",
    "\n",
    "    pooled_real = F.avg_pool2d(real_part, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "    pooled_imag = F.avg_pool2d(imag_part, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "\n",
    "    xk_freqs_cis = torch.complex(pooled_real, pooled_imag)\n",
    "\n",
    "    # === end of pooling ===\n",
    "\n",
    "    xk_freqs_cis = xk_freqs_cis.view(f_b, f_head, f_c, height // sr_ratio, width // sr_ratio)\n",
    "\n",
    "    xk_freqs_cis = xk_freqs_cis.permute(0, 1, 3, 4, 2).reshape(\n",
    "      f_b, f_head, -1, f_c\n",
    "    )  # (1, 1 or num_heads, H*W/sr^2, C_per_head//2)\n",
    "\n",
    "  xq_out = torch.view_as_real(xq_ * xq_freqs_cis).flatten(\n",
    "    3\n",
    "  )  # (B, num_heads, N, C_per_head//2) -> (B, num_heads, N, C_per_head//2, 2) -> (B, num_heads, N, C_per_head)\n",
    "  xk_out = torch.view_as_real(\n",
    "    xk_ * xk_freqs_cis\n",
    "  ).flatten(\n",
    "    3\n",
    "  )  # (B, num_heads, H*W/sr^2, C_per_head//2) -> (B, num_heads, N/sr^2, C_per_head//2, 2) -> (B, num_heads, N/sr^2, C_per_head)\n",
    "  return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(\n",
    "    xk.device\n",
    "  )  # (B, num_heads, N, C_per_head), (B, num_heads, N/sr^2, C_per_head)\n",
    "\n",
    "\n",
    "class SegformerWithRoPEEfficientSelfAttention(nn.Module):\n",
    "  \"\"\"Efficient Self Attention with RoPE module.\"\"\"\n",
    "\n",
    "  def __init__(self, config, hidden_size, num_attention_heads, sequence_reduction_ratio):\n",
    "    super().__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.num_attention_heads = num_attention_heads\n",
    "\n",
    "    if self.hidden_size % self.num_attention_heads != 0:\n",
    "      msg = (\n",
    "        f\"The hidden size ({self.hidden_size}) is not a multiple of the number of attention \"\n",
    "        f\"heads ({self.num_attention_heads})\"\n",
    "      )\n",
    "      raise ValueError(msg)\n",
    "\n",
    "    self.attention_head_size = int(self.hidden_size / self.num_attention_heads)\n",
    "    self.all_head_size = self.num_attention_heads * self.attention_head_size  # == hidden_size\n",
    "\n",
    "    self.query = nn.Linear(self.hidden_size, self.all_head_size)  # (B, H*W, C) -> (B, H*W, C)\n",
    "    self.key = nn.Linear(self.hidden_size, self.all_head_size)  # (B, H*W, C) -> (B, H*W, C)\n",
    "    self.value = nn.Linear(self.hidden_size, self.all_head_size)  # (B, H*W, C) -> (B, H*W, C)\n",
    "\n",
    "    self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    self.sr_ratio = sequence_reduction_ratio\n",
    "    if sequence_reduction_ratio > 1:\n",
    "      self.sr = nn.Conv2d(\n",
    "        hidden_size, hidden_size, kernel_size=sequence_reduction_ratio, stride=sequence_reduction_ratio\n",
    "      )\n",
    "      self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "\n",
    "  def transpose_for_scores(self, hidden_states):\n",
    "    new_shape = hidden_states.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "    hidden_states = hidden_states.view(new_shape)\n",
    "    return hidden_states.permute(0, 2, 1, 3)\n",
    "\n",
    "  def forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    height,\n",
    "    width,\n",
    "    freqs_cis,  # added (N, C_per_head//2) or (num_heads, N, C_per_head//2)\n",
    "    output_attentions=False,\n",
    "  ):\n",
    "    query_layer = self.transpose_for_scores(\n",
    "      self.query(hidden_states)\n",
    "    )  # (B, H*W, C) -> (B, H*W, C) -> (B, num_attention_heads, H*W, C_per_head)\n",
    "\n",
    "    if self.sr_ratio > 1:\n",
    "      batch_size, seq_len, num_channels = hidden_states.shape  # (B, H*W, C)\n",
    "      # Reshape to (batch_size, num_channels, height, width)\n",
    "      hidden_states = hidden_states.permute(0, 2, 1).reshape(batch_size, num_channels, height, width)  # (B, C, H, W)\n",
    "      # Apply sequence reduction\n",
    "      hidden_states = self.sr(hidden_states)  # (B, C, H, W) -> (B, C, H/sr, W/sr)\n",
    "      # Reshape back to (batch_size, seq_len, num_channels)\n",
    "      hidden_states = hidden_states.reshape(batch_size, num_channels, -1).permute(\n",
    "        0, 2, 1\n",
    "      )  # (B, C, H/sr*W/sr) -> (B, H/sr*W/sr, C) = (B, H*W/sr^2, C)\n",
    "      hidden_states = self.layer_norm(hidden_states)\n",
    "\n",
    "    key_layer = self.transpose_for_scores(\n",
    "      self.key(hidden_states)\n",
    "    )  # (B, H*W, C) -> (B, H*W, C) -> (B, num_attention_heads, H*W, C_per_head)\n",
    "    value_layer = self.transpose_for_scores(\n",
    "      self.value(hidden_states)\n",
    "    )  # (B, H*W, C) -> (B, H*W, C) -> (B, num_attention_heads, H*W, C_per_head)\n",
    "\n",
    "    # === Apply RoPE ===\n",
    "\n",
    "    # input: xq, xk, sr_ratio, freqs_cis\n",
    "    #   query_layer: (B, num_attention_heads, H*W, C_per_head)\n",
    "    #   key_layer: (B, num_attention_heads, H*W/sr^2, C_per_head)\n",
    "    # output: (B, num_heads, N, C_per_head), (B, num_heads, N/sr^2, C_per_head)\n",
    "    query_layer, key_layer = apply_rotary_emb(\n",
    "      query_layer,\n",
    "      key_layer,\n",
    "      self.sr_ratio,\n",
    "      freqs_cis,\n",
    "      height,\n",
    "      width,\n",
    "    )\n",
    "\n",
    "    # === End of RoPE ===\n",
    "\n",
    "    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "    attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "\n",
    "    # Normalize the attention scores to probabilities.\n",
    "    attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    # This is actually dropping out entire tokens to attend to, which might\n",
    "    # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "    attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "    context_layer = torch.matmul(attention_probs, value_layer)\n",
    "\n",
    "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "    new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "    context_layer = context_layer.view(new_context_layer_shape)\n",
    "\n",
    "    outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "class SegformerWithRoPEAttention(nn.Module):\n",
    "  def __init__(self, config, hidden_size, num_attention_heads, sequence_reduction_ratio):\n",
    "    super().__init__()\n",
    "    self.self = SegformerWithRoPEEfficientSelfAttention(\n",
    "      config=config,\n",
    "      hidden_size=hidden_size,\n",
    "      num_attention_heads=num_attention_heads,\n",
    "      sequence_reduction_ratio=sequence_reduction_ratio,\n",
    "    )\n",
    "    self.output = SegformerSelfOutput(config, hidden_size=hidden_size)\n",
    "    self.pruned_heads = set()\n",
    "\n",
    "  # def prune_heads(self, heads):\n",
    "  #     if len(heads) == 0:\n",
    "  #         return\n",
    "  #     heads, index = find_pruneable_heads_and_indices(\n",
    "  #         heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "  #     )\n",
    "\n",
    "  #     # Prune linear layers\n",
    "  #     self.self.query = prune_linear_layer(self.self.query, index)\n",
    "  #     self.self.key = prune_linear_layer(self.self.key, index)\n",
    "  #     self.self.value = prune_linear_layer(self.self.value, index)\n",
    "  #     self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "  #     # Update hyper params and store pruned heads\n",
    "  #     self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "  #     self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "  #     self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "  def forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    height,\n",
    "    width,\n",
    "    freqs_cis,  # added (N, C_per_head//2) or (num_heads, N, C_per_head//2)\n",
    "    output_attentions=False,\n",
    "  ):\n",
    "    self_outputs = self.self(hidden_states, height, width, freqs_cis, output_attentions)\n",
    "\n",
    "    attention_output = self.output(self_outputs[0], hidden_states)\n",
    "    outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "    return outputs\n",
    "\n",
    "\n",
    "class SegformerWithRoPELayer(nn.Module):\n",
    "  \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n",
    "\n",
    "  def __init__(self, config, hidden_size, num_attention_heads, drop_path, sequence_reduction_ratio, mlp_ratio):\n",
    "    super().__init__()\n",
    "    self.layer_norm_1 = nn.LayerNorm(hidden_size)\n",
    "    self.attention = SegformerWithRoPEAttention(\n",
    "      config,\n",
    "      hidden_size=hidden_size,\n",
    "      num_attention_heads=num_attention_heads,\n",
    "      sequence_reduction_ratio=sequence_reduction_ratio,\n",
    "    )\n",
    "    self.drop_path = SegformerDropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "    self.layer_norm_2 = nn.LayerNorm(hidden_size)\n",
    "    mlp_hidden_size = int(hidden_size * mlp_ratio)\n",
    "    self.mlp = SegformerMixFFN(config, in_features=hidden_size, hidden_features=mlp_hidden_size)\n",
    "\n",
    "  def forward(\n",
    "    self,\n",
    "    hidden_states,\n",
    "    height,\n",
    "    width,\n",
    "    freqs_cis,  # added (N, C_per_head//2) or (num_heads, N, C_per_head//2)\n",
    "    output_attentions=False,\n",
    "  ):\n",
    "    self_attention_outputs = self.attention(\n",
    "      self.layer_norm_1(hidden_states),  # in Segformer, layernorm is applied before self-attention\n",
    "      height,\n",
    "      width,\n",
    "      freqs_cis,\n",
    "      output_attentions=output_attentions,\n",
    "    )\n",
    "\n",
    "    attention_output = self_attention_outputs[0]\n",
    "    outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n",
    "\n",
    "    # first residual connection (with stochastic depth)\n",
    "    attention_output = self.drop_path(attention_output)\n",
    "    hidden_states = attention_output + hidden_states\n",
    "\n",
    "    mlp_output = self.mlp(self.layer_norm_2(hidden_states), height, width)\n",
    "\n",
    "    # second residual connection (with stochastic depth)\n",
    "    mlp_output = self.drop_path(mlp_output)\n",
    "    layer_output = mlp_output + hidden_states\n",
    "\n",
    "    outputs = (layer_output, *outputs)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "class SegformerWithRoPEEncoder(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.config = config\n",
    "\n",
    "    # stochastic depth decay rule\n",
    "    drop_path_decays = [x.item() for x in torch.linspace(0, config.drop_path_rate, sum(config.depths))]\n",
    "\n",
    "    # patch embeddings\n",
    "    embeddings = []\n",
    "    for i in range(config.num_encoder_blocks):  # 4 layers\n",
    "      embeddings.append(\n",
    "        SegformerOverlapPatchEmbeddings(\n",
    "          patch_size=config.patch_sizes[i],\n",
    "          stride=config.strides[i],\n",
    "          num_channels=config.num_channels if i == 0 else config.hidden_sizes[i - 1],\n",
    "          hidden_size=config.hidden_sizes[i],\n",
    "        )\n",
    "      )\n",
    "    self.patch_embeddings = nn.ModuleList(embeddings)\n",
    "\n",
    "    # Transformer blocks\n",
    "    blocks = []\n",
    "\n",
    "    cur = 0\n",
    "    for i in range(config.num_encoder_blocks):  # 4 layers\n",
    "      # each block consists of layers\n",
    "      layers = []\n",
    "      if i != 0:\n",
    "        cur += config.depths[i - 1]  # b0 [2, 2, 2, 2]\n",
    "      for j in range(config.depths[i]):\n",
    "        layers.append(\n",
    "          SegformerWithRoPELayer(\n",
    "            config,\n",
    "            hidden_size=config.hidden_sizes[i],\n",
    "            num_attention_heads=config.num_attention_heads[i],\n",
    "            drop_path=drop_path_decays[cur + j],\n",
    "            sequence_reduction_ratio=config.sr_ratios[i],\n",
    "            mlp_ratio=config.mlp_ratios[i],\n",
    "          )\n",
    "        )\n",
    "      blocks.append(nn.ModuleList(layers))\n",
    "\n",
    "    # axial: (encoder_blocks, H*W, dim_per_head//2 )\n",
    "    # mixed: (encoder_blocks, transformer_blocks, 2, num_blocks, num_heads, dim//2)\n",
    "    freqs = []\n",
    "    embedding_size = config.image_size\n",
    "    self.compute_cis = []\n",
    "\n",
    "    # === compute frequency ===\n",
    "    for i in range(config.num_encoder_blocks):  # 4\n",
    "      embedding_size = (\n",
    "        (embedding_size + 2 * (config.patch_sizes[i] // 2) - config.patch_sizes[i]) // config.strides[i]\n",
    "      ) + 1\n",
    "\n",
    "      if self.config.rope_mixed:\n",
    "        compute_cis = partial(compute_mixed_cis, num_heads=config.num_attention_heads[i])\n",
    "        self.compute_cis.append(compute_cis)\n",
    "\n",
    "        f = []  # (blocks, 2, num_heads, C_per_heads//2)\n",
    "\n",
    "        for _ in range(config.depths[i]):\n",
    "          f.append(\n",
    "            init_random_2d_freqs(\n",
    "              dim=config.hidden_sizes[i] // config.num_attention_heads[i],\n",
    "              num_heads=config.num_attention_heads[i],\n",
    "              theta=config.rope_theta,\n",
    "            )  # (2, num_heads, C_per_heads//2)\n",
    "          )\n",
    "\n",
    "        # (2, num_heads, C_per_heads//2)[num_blocks] -> (2, num_blocks, num_heads, C_per_heads // 2) -> (2, config.depths[i], num_blocks * num_heads * C_per_heads // (2 * config.depths[i]))\n",
    "        # -> (2, config.depths[i], C//2)\n",
    "        f = torch.stack(f, dim=1).view(2, config.depths[i], -1)\n",
    "\n",
    "        freqs.append(nn.Parameter(f.clone()))\n",
    "\n",
    "        _t_x, _t_y = init_t_xy(end_x=embedding_size, end_y=embedding_size)\n",
    "\n",
    "        self.register_buffer(f\"t_x_{i}\", _t_x)\n",
    "        self.register_buffer(f\"t_y_{i}\", _t_y)\n",
    "\n",
    "      else:\n",
    "        compute_cis = partial(\n",
    "          compute_axial_cis,\n",
    "          theta=self.config.rope_theta,\n",
    "          dim=config.hidden_sizes[i] // config.num_attention_heads[i],\n",
    "        )\n",
    "        self.compute_cis.append(compute_cis)\n",
    "\n",
    "        freqs_cis = compute_cis(end_x=embedding_size, end_y=embedding_size)  # (N, C_per_head//2)\n",
    "\n",
    "        freqs.append(freqs_cis)\n",
    "\n",
    "    if self.config.rope_mixed:\n",
    "      self.freqs = nn.ParameterList(freqs)\n",
    "    else:\n",
    "      self.freqs = freqs\n",
    "\n",
    "    # === end of compute frequency ===\n",
    "\n",
    "    self.block = nn.ModuleList(blocks)\n",
    "\n",
    "    # Layer norms\n",
    "    self.layer_norm = nn.ModuleList([nn.LayerNorm(config.hidden_sizes[i]) for i in range(config.num_encoder_blocks)])\n",
    "\n",
    "  def forward(\n",
    "    self,\n",
    "    pixel_values: torch.FloatTensor,\n",
    "    output_attentions: bool | None = False,\n",
    "    output_hidden_states: bool | None = False,\n",
    "    return_dict: bool | None = True,\n",
    "  ) -> tuple | BaseModelOutput:\n",
    "    all_hidden_states = () if output_hidden_states else None\n",
    "    all_self_attentions = () if output_attentions else None\n",
    "\n",
    "    batch_size = pixel_values.shape[0]\n",
    "\n",
    "    hidden_states = pixel_values\n",
    "    for idx, x in enumerate(zip(self.patch_embeddings, self.block, self.layer_norm, strict=False)):\n",
    "      embedding_layer, block_layer, norm_layer = x\n",
    "\n",
    "      # first, obtain patch embeddings\n",
    "      hidden_states, height, width = embedding_layer(hidden_states)  # (B, H*W, C)\n",
    "\n",
    "      # === compute cis ===\n",
    "\n",
    "      if self.config.rope_mixed:\n",
    "        t_x = getattr(self, f\"t_x_{idx}\")\n",
    "        t_y = getattr(self, f\"t_y_{idx}\")\n",
    "\n",
    "        compute_cis = self.compute_cis[idx]\n",
    "\n",
    "        # freqs_cis: (2, config.depths[i], C//2) -> # (depth, num_heads, H*W, C//2)\n",
    "        freqs_cis = compute_cis(freqs=self.freqs[idx], t_x=t_x, t_y=t_y)\n",
    "      else:\n",
    "        freqs_cis = self.freqs[idx].to(\n",
    "          pixel_values.device\n",
    "        )  # (encoder_blocks, H*W, dim_per_head//2 )[idx] -> (N, C_per_head//2)\n",
    "\n",
    "      # === end of compute cis ===\n",
    "\n",
    "      # second, send embeddings through blocks\n",
    "      for i, blk in enumerate(block_layer):\n",
    "        layer_outputs = blk(\n",
    "          hidden_states, height, width, freqs_cis[i] if self.config.rope_mixed else freqs_cis, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = layer_outputs[0]\n",
    "\n",
    "        if output_attentions:\n",
    "          all_self_attentions = (*all_self_attentions, layer_outputs[1])\n",
    "\n",
    "      # third, apply layer norm\n",
    "      hidden_states = norm_layer(hidden_states)\n",
    "\n",
    "      # fourth, optionally reshape back to (batch_size, num_channels, height, width)\n",
    "      if idx != len(self.patch_embeddings) - 1 or (\n",
    "        idx == len(self.patch_embeddings) - 1 and self.config.reshape_last_stage\n",
    "      ):\n",
    "        hidden_states = hidden_states.reshape(batch_size, height, width, -1).permute(0, 3, 1, 2).contiguous()\n",
    "      if output_hidden_states:\n",
    "        all_hidden_states = (*all_hidden_states, hidden_states)\n",
    "\n",
    "    if not return_dict:\n",
    "      return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n",
    "    return BaseModelOutput(\n",
    "      last_hidden_state=hidden_states,\n",
    "      hidden_states=all_hidden_states,\n",
    "      attentions=all_self_attentions,\n",
    "    )\n",
    "\n",
    "\n",
    "class SegformerWithRoPEModel(SegformerModel):\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "    self.encoder = SegformerWithRoPEEncoder(config)\n",
    "    self.post_init()\n",
    "\n",
    "\n",
    "class SegformerWithRoPEForSemanticSegmentation(SegformerForSemanticSegmentation):\n",
    "  def __init__(self, config):\n",
    "    super().__init__(config)\n",
    "    self.segformer = SegformerWithRoPEModel(config)\n",
    "    self.post_init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hJlPPm4DKC_r"
   },
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "from transformers import SegformerConfig\n",
    "\n",
    "\n",
    "class SegformerWithRoPEConfig(SegformerConfig):\n",
    "  \"\"\"The configuration class of a Segformer model with a RoPE module.\n",
    "\n",
    "  Args:\n",
    "      SegformerConfig (_type_): _description_\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  model_type = \"segformer-with-rope\"\n",
    "\n",
    "  def __init__(self, rope_theta: float = 100.0, image_size: int = 512, *, rope_mixed: bool = False, **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.rope_theta = rope_theta\n",
    "    self.image_size = image_size\n",
    "    self.rope_mixed = rope_mixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrsZbn9DKIXT"
   },
   "source": [
    "## 4.2 Instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3HWpFIHKSUS"
   },
   "outputs": [],
   "source": [
    "configuration = SegformerConfig(num_labels=150, id2label=id2label, label2id=label2id)\n",
    "model = SegformerForSemanticSegmentation(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4QMBdRayKIGa"
   },
   "outputs": [],
   "source": [
    "configuration = SegformerWithRoPEConfig(num_labels=150, id2label=id2label, label2id=label2id, rope_mixed=False)\n",
    "model = SegformerWithRoPEForSemanticSegmentation(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GBGPh7F6Kp6E"
   },
   "source": [
    "# 5. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S9YslGPfKrz7"
   },
   "outputs": [],
   "source": [
    "!wandb login --relogin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZ_s_Kr1KtTG"
   },
   "outputs": [],
   "source": [
    "NAME = \"segformer-with-ropoe-b0-mixed\"\n",
    "DIRECTORY = \"\"\n",
    "\n",
    "TRAIN_DATASET_SIZE = len(train_dataset)\n",
    "EVAL_DATASET_SIZE = len(validation_dataset)\n",
    "\n",
    "NUM_EPOCK = 128\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "EVAL_BATCH_SIZE = 16\n",
    "\n",
    "GRADIENT_ACCUMULATION = 1\n",
    "EVAL_ACCUMULATION = 4\n",
    "\n",
    "EVAL_LATE = 5 / 100\n",
    "\n",
    "EVAL_STEPS = int(TRAIN_DATASET_SIZE // (TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION) * NUM_EPOCK * EVAL_LATE)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "  run_name=NAME,\n",
    "  output_dir=f\"{DIRECTORY}/outputs/{NAME}\",\n",
    "  logging_dir=f\"{DIRECTORY}/logs/{NAME}\",\n",
    "  learning_rate=1e-3,\n",
    "  lr_scheduler_type=\"polynomial\",\n",
    "  warmup_ratio=0.1,\n",
    "  num_train_epochs=NUM_EPOCK,\n",
    "  per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "  per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "  eval_accumulation_steps=EVAL_ACCUMULATION,\n",
    "  gradient_accumulation_steps=GRADIENT_ACCUMULATION,\n",
    "  save_total_limit=3,\n",
    "  eval_strategy=\"steps\",\n",
    "  save_strategy=\"steps\",\n",
    "  save_steps=EVAL_STEPS,\n",
    "  eval_steps=EVAL_STEPS,\n",
    "  logging_steps=EVAL_STEPS,\n",
    "  remove_unused_columns=False,\n",
    "  dataloader_num_workers=4,\n",
    "  # fp16=True,\n",
    "  # use_cpu=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset=train_dataset,\n",
    "  eval_dataset=validation_dataset,\n",
    "  compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "ic(EVAL_STEPS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M5tS4mqoLG7g"
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wDzFA6dsLIMj"
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdlGRmn5LIqA"
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN9+VmgqdxqrPqAbmL8SBk+",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
